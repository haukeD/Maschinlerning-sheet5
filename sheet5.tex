%inizative bildverarbeitung
%llncs    article
\documentclass[a4paper,parskip=full-]{article}

\usepackage{amsmath}
\usepackage{amssymb}

%http://de.wikibooks.org/wiki/LaTeX-W%C3%B6rterbuch:_Anf%C3%BChrungszeichen
\usepackage[autostyle=true,german=quotes]{csquotes}

\usepackage{subcaption}
\usepackage{float}
\restylefloat{figure}
\usepackage{graphicx}  % images
%\usepackage{qtree} % for trees
%auch für Bäume
%http://tex.stackexchange.com/questions/183866/tree-with-six-or-more-children
\usepackage{tikz, tikz-qtree}
\usepackage{pgfplots}
\pgfplotsset{grid style={dashed,gray}}
\usetikzlibrary{automata,topaths,plotmarks}
\usepackage{lmodern}  %THE tex font :)
\usepackage{url}  %urls in references
%\usepackage{prettyref}
%\usepackage{pstricks} %graphicv2
\usepackage{cite}
\usepackage{enumerate}
\usepackage{multicol}
\usepackage{xspace}

%http://www.kkittel.de/wiki/doku.php?id=tabellen:farbige_zellen
\usepackage{colortbl}

\usepackage{algorithmic} 
\usepackage{wasysym}
\usepackage[ngerman]{babel}
\usepackage[utf8]{inputenc} % Zeichenkodierung   utf8   latin1
%\include{biblio} % references
\usepackage{listings}                           % for source code inclusion
\usepackage{multirow} 
\usepackage{caption}
\usepackage{wrapfig}
\usepackage{color}

\usepackage{fancyvrb }

%absatz
\usepackage{setspace} 

%breaking math formulars automaticly
%http://tex.stackexchange.com/questions/3782/how-can-i-split-an-equation-over-two-lines
\usepackage{breqn}

%für kurze Enumarates wie i,I a, A etc.
\usepackage[shortlabels]{enumitem}

%Durchstreichungen
%\cancel
%http://de.wikibooks.org/wiki/LaTeX-Kompendium:_F%C3%BCr_Mathematiker#Durchstreichungen
\usepackage{cancel}

%Für Römische Zahlen
\usepackage{enumitem}
%\usepackage{romannum}%stdclsdv

%Durchstreicen möglich
\usepackage[normalem]{ulem}
 
%Bessere Brüche
\usepackage{nicefrac}

%bookmarks
%\usepackage[pdftex,bookmarks=true]{hyperref}
%[pdftex,bookmarks=true,bookmarksopen,bookmarksdepth=2]
\usepackage{hyperref}
%\usepackage{scrhack}

%fußnoten
\usepackage{footnote}
\usepackage{caption} 

\usepackage{geometry}
\geometry{verbose,a4paper,tmargin=25mm,bmargin=25mm,lmargin=15mm,rmargin=20mm}

%randnotiz
\newcommand\mpar[1]{\marginpar {\flushleft\sffamily\small #1}}
\setlength{\marginparwidth}{3cm}

%svg Grafiken
%http://tex.stackexchange.com/questions/122871/include-svg-images-with-the-svg-package
%\usepackage{svg}

\usepackage{pgf}

%http://tex.stackexchange.com/questions/48653/making-subsections-be-numbered-with-a-b-c
\usepackage{chngcntr}
\counterwithin{subsection}{section}

%Sektions nicht Nummerrrieren (<=> section*{...})
% \makeatletter
% \renewcommand\@seccntformat[1]{}
% \makeatother
\setcounter{secnumdepth}{0}

\title{Machine Learning \\
Exercise sheet 4}

\author{Gruppe 9: \\Hauke Wree and Fridtjof Schulte Steinberg}

\newcommand{\R}[0]{{\mathbb{R}}}

\newcommand{\N}[0]{{\mathbb{N}}}
\newcommand{\C}[0]{{\mathbb{C}}}
\newcommand{\K}[0]{{\mathbb{K}}}
\newcommand{\lF}[0]{{\mathcal{l}}}
\newcommand{\I}[0]{{\mathcal{I}}}
\newcommand{\nhnh}[0]{{\frac{n}{2} \times \frac{n}{2}}} %nice
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
%\newcommand{\rm}[1]{\romannumeral #1}
\newcommand{\RM}[1]{\MakeUppercase{\romannumeral #1{.}}} 

\renewcommand \thesubsection{\alph{subsection}}

\begin{document}

\maketitle

\section{Exercise 1 (Linear Discriminant Analysis LDA):}
%\subsection{a)}
%
%\subsubsection{Behauptung}
%Die Summe der within-class scatter $S_W$ 
%und der between-class scatter $S_B$ 
%ist gleich der Kovariance Matrix $\Sigma:\Sigma = S_W + S_B$
%
%\subsubsection{Beweis}
%Der within-class scatter $S_W$ und der between-class scatter $S_B$ ist gegeben mit:
%
%$$
%S_W = \frac{1}{N'} \sum^K_{k=1} \sum_{x_n \in C_k} (x_n-m_k)(x_n-m_k)^T
%$$
%
%$$
%S_B = \frac{1}{N'} \sum^K_{k=1} N_k (m_k-m)(m_k-m)^T
%$$
%Die Kovarianzmatrix ist gegeben mit:
%$$
%\Sigma =  \frac{1}{N} \sum^N_{n=1} (x_n-m)(x_n-m)^T
%$$

\subsection{b)}
\subsubsection{Behauptung}
Für ein zwei Klassen Problem, ist die between-class scatter $S_B$ proportional zur differenz der beiden Mittelwertvektoren multipliziert mit ihrem transponiertem, es gilt somit:
$$
S_B \sim (\vec{m}_1 - \vec{m}_2) \cdot \left( \vec{m}_1 - \vec{m}_2 \right)^T
$$

\subsubsection{Beweis}
Der  between-class scatter $S_B$ ist definiert mit:
$$
S_B = \frac{1}{N'} \sum^K_{k=1} N_k  \left( \vec{m}_1 - \vec{m}_2 \right)^T \cdot (\vec{m}_1 - \vec{m}_2)
$$
da $K=2$ gilt:
$$
S_B = \frac{1}{N'} \left( N_1 (\vec{m}_1 - \vec{m})^T (\vec{m}_1 - \vec{m}) + 
N_2 (\vec{m}_2 - \vec{m})^T (\vec{m}_2 - \vec{m})  \right)
$$
und 
$$
\vec{m} = \frac{\vec{m}_1 + \vec{m}_2}{2}
$$
somit gilt weiter für $S_B$
\begin{equation*}
\begin{aligned}
S_B = & \frac{1}{N'} 
\left( 
N_1 (\vec{m}_1 - \frac{\vec{m}_1 + \vec{m}_2}{\vec{m}})^T (\vec{m}_1 - \frac{\vec{m}_1 + \vec{m}_2}{\vec{m}}) + 
N_2 (\vec{m}_2 - \frac{\vec{m}_1 + \vec{m}_2}{\vec{m}})^T (\vec{m}_2 - \frac{\vec{m}_1 + \vec{m}_2}{\vec{m}})  
\right) \\
= & \frac{1}{N'} \left(
\frac{N_1}{4} \left( \vec{m}_1 - \vec{m}_2 \right)^T \cdot (\vec{m}_1 - \vec{m}_2) - 
\frac{N_2}{4} \left( \vec{m}_1 - \vec{m}_2 \right)^T \cdot (\vec{m}_1 - \vec{m}_2)
\right) \\
= & \frac{N_1 - N_2}{4 N'} 
\left(
\left(\vec{m}_1 - \vec{m}_2 \right)^T \cdot (\vec{m}_1 - \vec{m}_2)
\right)
\sim (\vec{m}_1 - \vec{m}_2) \cdot \left( \vec{m}_1 - \vec{m}_2 \right)^T
\end{aligned}
\end{equation*}

\subsection{c)}
Class 1: Data points $X_1 = \{(4,1),(2,4),(2,3),(3,6),(4,4) \}$ \\
Class 2: Data points $X_2 = \{(9,10),(6,8),(9,5),(8,7),(10,8) \}$ \\

$$
m_1 = 
\begin{pmatrix}
3 \\ 3,6
\end{pmatrix}
m_2 = \begin{pmatrix}
8,4 \\ 7,6
\end{pmatrix}
m = \begin{pmatrix}
5,7 \\ 5,6
\end{pmatrix}
$$

\begin{multline*}
%\begin{split}
S_W =  \frac{1}{N'} \sum^K_{k=1} \sum_{x_n \in C_k} (x_n-m_k)(x_n-m_k)^T \\
=  \frac{1}{5+5} \Biggl(
\left( \begin{pmatrix} 4 \\ 1 \end{pmatrix} - \begin{pmatrix} 3 \\ 3,6 \end{pmatrix} \right)^T \cdot
\left( \begin{pmatrix} 4 \\ 1 \end{pmatrix} - \begin{pmatrix} 3 \\ 3,6 \end{pmatrix} \right) + 
\left( \begin{pmatrix} 2 \\ 4 \end{pmatrix} - \begin{pmatrix} 3 \\ 3,6 \end{pmatrix} \right)^T \cdot
\left( \begin{pmatrix} 2 \\ 4 \end{pmatrix} - \begin{pmatrix} 3 \\ 3,6 \end{pmatrix} \right) + \\
\left( \begin{pmatrix} 2 \\ 3 \end{pmatrix} - \begin{pmatrix} 3 \\ 3,6 \end{pmatrix} \right)^T \cdot
\left( \begin{pmatrix} 2 \\ 3 \end{pmatrix} - \begin{pmatrix} 3 \\ 3,6 \end{pmatrix} \right) + 
\left( \begin{pmatrix} 3 \\ 6 \end{pmatrix} - \begin{pmatrix} 3 \\ 3,6 \end{pmatrix} \right)^T \cdot
\left( \begin{pmatrix} 3 \\ 6 \end{pmatrix} - \begin{pmatrix} 3 \\ 3,6 \end{pmatrix} \right) + \\
\left( \begin{pmatrix} 4 \\ 4 \end{pmatrix} - \begin{pmatrix} 3 \\ 3,6 \end{pmatrix} \right)^T \cdot
\left( \begin{pmatrix} 4 \\ 4 \end{pmatrix} - \begin{pmatrix} 3 \\ 3,6 \end{pmatrix} \right) + 
%m2
\left( \begin{pmatrix}  9 \\ 10 \end{pmatrix} - \begin{pmatrix} 8,4 \\ 7,6 \end{pmatrix} \right)^T \cdot
\left( \begin{pmatrix}  9 \\ 10 \end{pmatrix} - \begin{pmatrix} 8,4 \\ 7,6 \end{pmatrix} \right) +
\left( \begin{pmatrix}  6 \\  8 \end{pmatrix} - \begin{pmatrix} 8,4 \\ 7,6 \end{pmatrix} \right)^T \cdot
\left( \begin{pmatrix}  6 \\  8 \end{pmatrix} - \begin{pmatrix} 8,4 \\ 7,6 \end{pmatrix} \right) + \\
\left( \begin{pmatrix}  9 \\  5 \end{pmatrix} - \begin{pmatrix} 8,4 \\ 7,6 \end{pmatrix} \right)^T \cdot
\left( \begin{pmatrix}  9 \\  5 \end{pmatrix} - \begin{pmatrix} 8,4 \\ 7,6 \end{pmatrix} \right) + 
\left( \begin{pmatrix}  5 \\  7 \end{pmatrix} - \begin{pmatrix} 8,4 \\ 7,6 \end{pmatrix} \right)^T \cdot
\left( \begin{pmatrix}  5 \\  7 \end{pmatrix} - \begin{pmatrix} 8,4 \\ 7,6 \end{pmatrix} \right) + \\
\left( \begin{pmatrix} 10 \\  8 \end{pmatrix} - \begin{pmatrix} 8,4 \\ 7,6 \end{pmatrix} \right)^T \cdot
\left( \begin{pmatrix} 10 \\  8 \end{pmatrix} - \begin{pmatrix} 8,4 \\ 7,6 \end{pmatrix} \right) 
\Biggr) \\
= \frac{1}{10} \Biggl(
\begin{pmatrix} 1 &  -2,6 \end{pmatrix} \cdot
\begin{pmatrix} 1 \\ -2,6 \end{pmatrix} +
\begin{pmatrix} -1 &  0,4 \end{pmatrix} \cdot
\begin{pmatrix} -1 \\ 0,4 \end{pmatrix} +
\begin{pmatrix} -1 &  -0,6 \end{pmatrix} \cdot
\begin{pmatrix} -1 \\-0,6 \end{pmatrix} + \\
\begin{pmatrix} 0 &  2,4 \end{pmatrix} \cdot
\begin{pmatrix} 0 \\ 2,4 \end{pmatrix} +
\begin{pmatrix} 1 &  0,4 \end{pmatrix} \cdot
\begin{pmatrix} 1 \\ 0,4 \end{pmatrix} +
%m2
\begin{pmatrix} 0,6 &  2,4 \end{pmatrix} \cdot
\begin{pmatrix} 0,6 \\ 2,4 \end{pmatrix} +
\begin{pmatrix} -2,4 &  0,4 \end{pmatrix} \cdot
\begin{pmatrix} -2,4 \\ 0,4 \end{pmatrix} +
\begin{pmatrix} 0,6 &  -2,6 \end{pmatrix} \cdot
\begin{pmatrix} 0,6 \\ -2,6 \end{pmatrix} + \\
\begin{pmatrix} -0,4 &  -0,6 \end{pmatrix} \cdot
\begin{pmatrix} -0,4 \\ -0,6 \end{pmatrix} +
\begin{pmatrix} 1,6 &  0,4 \end{pmatrix} \cdot
\begin{pmatrix} 1,6 \\ 0,4 \end{pmatrix}  
\Biggr) = \\ \frac{1}{10} \Biggl(
\begin{pmatrix} 1 & -2,6 \\ -2,6 & 6,76 \end{pmatrix} +
\begin{pmatrix} 1 & -0,4 \\ -0,4 & 0,16 \end{pmatrix} +
\begin{pmatrix} 1 & 0,6 \\ 0,6 & 0,36 \end{pmatrix} +
\begin{pmatrix} 0 & 0 \\ 0 & 5,76 \end{pmatrix} +
\begin{pmatrix} 1 & 0,4 \\ 0,4 & 0,16 \end{pmatrix} + \\
\begin{pmatrix} 0,36 & 1,44 \\ 1,44 & 5,76 \end{pmatrix} +
\begin{pmatrix} 5,76 & -0,96 \\ -0,96 & 0,16 \end{pmatrix} +
\begin{pmatrix} 0,36 & -1,56 \\ -1,56 & 6,76 \end{pmatrix} +
\begin{pmatrix} 0,16 & 0,24 \\ 0,24 & 0,36 \end{pmatrix} +
\begin{pmatrix} 2,56 & 0,64 \\ 0,64 & 0,16 \end{pmatrix} \Biggr) \\
= \begin{pmatrix} 
1,32 & -0,22 \\ 
-0,22 & 2,64
\end{pmatrix} = \frac{1}{550}
\begin{pmatrix} 
6 & 1 \\ 1 & 12
\end{pmatrix}
%\end{split}
\end{multline*}

\begin{multline*}
S_B = \begin{pmatrix} 7,29 & 5,4 \\ 5,4 & 4 \end{pmatrix}
\end{multline*}


$$
S_W^{-1} = \left(
\begin{array}{cc}
0.76824584 & 0.06402049 \\
0.06402049 & 0.38412292
\end{array}
\right)
$$

$$
S_W^{-1} S_B \vec{w} = \lambda \vec{w} \Leftrightarrow
\left(
\begin{array}{cc}
 5.94622279 &  4.40460948 \\
 2.54097311 &  1.8822023 \\
\end{array}
\right) \cdot \vec{w} = \lambda \vec{w} = 0 \Leftrightarrow
\left(
\begin{array}{cc}
 5.94622279 - \lambda & 4.40460948 \\
 2.54097311           & 1.8822023 - \lambda \\
\end{array}
\right) \cdot \vec{w} = 0
$$
Wir berechnen jetzt die Eigenvektoren.

$$
Det\left(
\begin{array}{cc}
 5.94622279 - \lambda & 4.40460948 \\
 2.54097311           & 1.8822023 - \lambda \\
\end{array}
\right) = \lambda^2-7,82843
$$

Dann sind die EigenvektorenX

\begin{itemize}
\item $2,79793$
$$
\vec{w} = 
\begin{pmatrix} -0,378157864067 \\ 1 \end{pmatrix}
$$
\item $-2,79793$
$$
\vec{w} = 
\begin{pmatrix} 0,294792452718 \\ 1 \end{pmatrix}
$$
\end{itemize}

Wir transformieren die Datenpunkte.

$$
z_1
 = 
-0,51263145627, 
z_2
 = 
3,24368427187,
z_3 
 = 
2,24368427187 
$$
$$
z_4
 = 
4,8655264078,
z_5
 = 
2,48736854373,
z_6
 = 
6,59657922339 $$ $$
z_7
 = 
5,7310528156,
z_8
 = 
1,59657922339,
z_9
 = 
3,97473708746,
z_{10}
 = 
4,21842135933,
$$

\subsection{d)}

\begin{figure}[H]
    \def\svgwidth{\columnwidth}
	\input{1d.pdf_tex}
	\label{fig:1d}
	\caption{1d}
\end{figure}

\subsection{e)}

\begin{figure}[H]
    \def\svgwidth{\columnwidth}
	\input{1e.pdf_tex}
	\label{fig:1e}
	\caption{1e}
\end{figure}

Wie in zu sehen ist, wurden die Punkte auf einer geraden projiziert, in der jetzt die 3 Klassen einfach trennbar sind.

\subsection{f)}

\subsubsection{1d}

\begin{figure}[H]
    \def\svgwidth{\columnwidth}
	\input{1f1d.pdf_tex}
	\label{fig:1f1d}
	\caption{1f1d}
\end{figure}

\subsubsection{1d}

\begin{figure}[H]
    \def\svgwidth{\columnwidth}
	\input{1f2d.pdf_tex}
	\label{fig:1f2d}
	\caption{1f2d}
\end{figure}

\end{document}